:orphan:

:py:mod:`PG-SUI.impute.impute`
==============================

.. py:module:: PG-SUI.impute.impute


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   PG-SUI.impute.impute.Impute




.. py:class:: Impute(clf: Union[str, Callable], clf_type: str, kwargs: Dict[str, Any])

   Class to impute missing data from the provided classifier.

   The Impute class will either run a variational autoencoder or IterativeImputer with the provided estimator. The settings for the provided estimator should be provided as the ``kwargs`` argument as a dictionary object with the estimator's keyword arguments as the keys and the corresponding values. E.g., ``kwargs={"n_jobs", 4, "initial_strategy": "populations"}``. ``clf_type`` just specifies either "classifier" or "regressor". "regressor" is primarily just for quick and dirty testing.

   Once the Impute class is initialized, the imputation should be performed with ``fit_predict()``.

   The imputed data can then be written to a file with ``write_imputed()``

   Args:
       clf (str or Callable estimator object): The estimator object to use. If using a variational autoencoder, the provided value should be "VAE". Otherwise, it should be a callable estimator object that is compatible with scikit-learn's IterativeImputer.

       clf_type (str): Specify whether to use a "classifier" or "regressor". The "regressor" option is just for quick and dirty testing, and "classifier" should almost always be used.

       kwargs (Dict[str, Any]): Settings to use with the estimator. The keys should be the estimator's keywords, and the values should be their corresponding settings.

   Raises:
       TypeError: Check whether the ``gridparams`` values are of the correct format if ``ga=True`` or ``ga=False``.

   Examples:
       # Don't use parentheses after estimator object.
       >>> imputer = Impute(
               sklearn.ensemble.RandomForestClassifier,
               "classifier",
               {
                   "n_jobs": 4,
                   "initial_strategy": "populations",
                   "max_iter": 25,
                   "n_estimators": 100,
                   "ga": True
               }
           )

       >>> self.imputed, self.best_params = imputer.fit_predict(df)

       >>> imputer.write_imputed(self.imputed)

   .. py:method:: fit_predict(self, X: pandas.DataFrame) -> Tuple[pandas.DataFrame, Dict[str, Any]]

      Fit and predict imputations with IterativeImputer(estimator).

      Fits and predicts imputed 012-encoded genotypes using IterativeImputer with any of the supported estimator objects. If ``gridparams=None``, then a grid search is not performed. If ``gridparams!=None``, then a RandomizedSearchCV is performed on a subset of the data and a final imputation is done on the whole dataset using the best found parameters.

      Args:
          X (pandas.DataFrame): DataFrame with 012-encoded genotypes.

      Returns:
          pandas.DataFrame: DataFrame with missing 012-encoded genotypes imputed.

          Dict[str, Any]: Best parameters found during grid search.


   .. py:method:: write_imputed(self, data: Union[pandas.DataFrame, numpy.ndarray, List[List[int]]]) -> None

      Save imputed data to disk as a CSV file.

      Args:
          data (pandas.DataFrame, numpy.ndarray, or List[List[int]]): Object returned from ``fit_predict()``.

      Raises:
          TypeError: Must be of type pandas.DataFrame, numpy.array, or List[List[int]].


   .. py:method:: read_imputed(self, filename: str) -> pandas.DataFrame

      Read in imputed CSV file as formatted by write_imputed.

      Args:
          filename (str): Name of imputed CSV file to be read.

      Returns:
          pandas.DataFrame: Imputed data as DataFrame of 8-bit integers.


   .. py:method:: df2chunks(self, df: pandas.DataFrame, chunk_size: Union[int, float]) -> List[pandas.DataFrame]

      Break up pandas.DataFrame into chunks and impute chunks.

      If set to 1.0 of type float, then returns only one chunk containing all the data.

      Args:
          df (pandas.DataFrame): DataFrame to split into chunks.

          chunk_size (int or float): If type is integer, then breaks DataFrame into ``chunk_size`` chunks. If type is float, breaks DataFrame up into ``chunk_size * len(df.columns)`` chunks.

      Returns:
          List[pandas.DataFrame]: List of pandas DataFrames of shape (n_samples, n_features_in_chunk).

      Raises:
          ValueError: ``chunk_size`` must be of type int or float.



