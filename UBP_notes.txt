UBP Gashler Paper Notes

MLP=Multi-layer perceptron
W=Weights

Impute missing attributes of X
X is an n x d matrix, where each of the d attributes may be continuous or categorical

Predict missing elements in X

X becomes the output
Set of latent vectors (V) are fed as inputs into the MLP
However, no examples from V are given in the training data, and both V and W must be trained using only the known elements in X.

Training this way causes the MLP to fit to a non-linear surface manifold sampled by X.
V then becomes a reduced-dimensional representation of X
V is an n x t matrix, with t much smaller than d.
MLP maps V -> X from low-dimensional to high-dimensional space.

UBP trains MLP with the known attributes in X with online backpropagation.

Computes gradient vector g to update the weights
Computes gradient vector h to update the input vector Vr (Xr,c is the element in row r, column c of X).

After training, the weights represent nonlinear components (analogous to principal components - non-linear PCA or matrix factorization).

UBP is comparable with the latter half of an autoencoder (map low-dimensional training data to original input features).

UBP algorithm:

i are the network units.
j are the network layers.

1. Let X be a given n x d matrix, which may have many missing elements. We seek to impute
values for these elements. n is the number of instances. d is the number of attributes.

2. Let V be a latent n x t matrix, where t < d.

3. If xr,c is the element at row r , column c in X, then  X[hat]r,c is the value predicted by the MLP
for this element when vr E V is fed forward into the MLP.

4. Let wij be the weight that feeds from unit i to unit j in the MLP.

5. For each network unit i on layer j , let Betaj,i be the net input into the unit, let  alphaj,i be the
output or activation value of the unit, and let Deltaj,i be an error term associated with the
unit.

6. Let l be the number of hidden layers in the MLP.

7. Let f be the activation function used in the MLP. Our implementation uses the logistic
function. f' refers to the derivative of the activation function.

8. Let g be a vector representing the gradient with respect to the weights of an MLP, such
that gi,j is the component of the gradient that is used to refine wi,j .

9. Let h be a vector representing the gradient with respect to the inputs of an MLP, such
that hi is the component of the gradient that is used to refine vr,i E vr.

More efficient to train with each known element individually.

Three-phase training:
1. Compute an initial estimate for the intrinsic vectors, V;
2. Compute an initial estimate for the network weights, W; and
3. Refine them both together.

All three train using stochastic gradient descent.

Algorithm 1 (UBP):

# Phase 1: Compute initial estimate for V
1. Intitialize each element V with small random values (draw from Normal distribution with a mean of 0 and SD of 0.01.
2. Let T be the weights of a single-layer perceptron.
3. Initialize each element in T with small random values. # Only used in phase 1 to assist the initial training of V.

# Set heuristic values to check for convergence.; n' is the initial learning rate; Convergence is when learning rate falls below n".
# Gamma specifies the amount of improvement that is expected after each epoch, or else the learning rate is decayed.
# lambda is a regularization term that is used during phases 1 and 2 to ensure the weights do not become excessively saturated before the final phase of training.
# No regularization is used in the final phase. Overfitting can be mitigated by limiting the number of hidden units.
4. n' <- 10^-2; n" <- 10^-4, gamma <- 10^-5, lambda <- 10^-4 


5. n <- n'; s' <- infinity # Set learning rate, n, to the initial value. S' is used to store the previous error score. Initialized to infinity because no error has been measured.
# Train V and T until convergence is detected. T may then be discarded.
6. while n > n":
	s <- train_epoch(X, T, gamma, true, 0, n)
	if 1 - s/s' < gamma then n <- n/2
	s' <- s
	
# Phase 2: Compute initial estimate for W. Differences from Phase 1: An MLP is used instead of a temporary single-layer perceptron; V is held constant during this phase.
10. Let W be the weights of a MLP with l hidden layers, l >= 0
11. initialize each element W with small random values.
12. n <- n'; s' <- infinity
13. while n > n":
	s <- train_epoch(X< W, lambda, false, l, n)
	if 1 - s/s' < gamma, then n <- n/2
	s' <- s
	
# # Phase 3: Train V and W together.
# The same MLP from phase 2 is used again, but V and W are both refined together. No regularization is used here.
17. n <- n'; s' <- infinity
18. while n > n":
	s <- train_epoch(X, W, 0, true, l, n)
	if 1 - s/s' < gamma then n <- n/2
	s' <- s
22.return V,W

Algorithm 2 - train_epoch(X, W, lambda, p, l, n)
1. for each known Xr,c E X in random order:
      Compute output/activation weights (alpha) by forward-propagating vr into an MLP with weights W.
      DeltaC (error for column) <- (Xr,c - Alphac)f'(BetaC) # BetaC is net input into unit; Compute predicted value for the presented element given the current vr. Should only propagate values into output unit c.
	  for each hidden unit i feeding into output unit c:
	      Deltai <- wi,c * Deltac * f'(Betai) # Compute error term for output unit c and each hiddent unit in the network. Activation of the other output units is not computed here. Thus, error for those is 0.
	  for each hiddent unit j in an earlier hidden layer (in backward order):
	      Deltaj <- SigmaK(Wj,k * Deltak * f'(Betaj)
		  
	  # Refine W by descending the gradient with a step size of n.
	  for each Wi,j E W:
	      gi,j <- -Deltaj * Alphai
	    W <- W - n(G + lambda * W)
		
		# Only refine V in phases 1 and 3.
		if p == True then:
		    for i from 0 to t - 1:
				# refine V by descending the gradient with a step size of n.
			   if l = 0:
			       then hi <- -Wi,c * Deltac
				else:
				    hi <- -Sigmaj(Wi,j * Deltaj)
	        vr <- vr - n(h + lambda * vr)
	s <- measure RMSE with X # Compute RMSE of the MLP for each known element in X. To enable UBP to process nominal (categorical) attributes, we convert such values to a vector representing membership weight in each category.
	# E.g., for a given value of "cat" in the value set {"mouse", "cat", "dog"}, it is represented with the vector {0, 1, 0}. Unknown values in this attribute are converted to three unknown real values,
	# requiring the algorithm to make three predictions. After missing values are imputed, we convert the data back to their original form by finding the mode of each categorical distribution.
	# E.g., the predicted vactor {0.4, 0.25, 0.35} would be converted to a prediction of "mouse".
	return s

4. 







