# %% markdown
# # Run Super Deli API
#
# First, import the necessary modules
# %% codecell
import numpy as np
import pandas as pd

from read_input.read_input import GenotypeData
import read_input.impute as impute

from dim_reduction.dim_reduction import DimReduction
from dim_reduction.embed import *
from clustering.clustering import *
# %% markdown
# ## Load Data
#
# Then load the data into a GenotypeData object.
# The GenotypeData object is used throughout the API, and multiple useful attributes
# can be accessed from it.
#
# GenotypeData does the necessary file conversions.
# All you have to do is input a PHYLIP or STRUCTURE formatted file and a population map (popmap).
#
# If using a STRUCTURE file, Super Deli supports both the 2 rows per individual and 1 row per individual formats.
# Also, the structure file can have population IDs as the 2nd column, but if not, then the population IDs need
# to be supplied with a popmap file.
# %% codecell
filename="./example_data/structure_files/test.nopops.str"
filetype="structure2row"
popmapfile="./example_data/popmaps/test.popmap"

data = GenotypeData(filename=filename, filetype=filetype, popmapfile=popmapfile)
# %% markdown
# ## Impute Missing Data
#
# Many of the machine learning algorithms require no missing data.
# Thus, we have implemented several imputation methods.
#
# Currently supported imputation methods include:
#
# 1. Global allele frequency
# 2. By-population allele frequency
# 3. K-Nearest Neighbors (Standard with Manual K)
# 4. K-Nearest Neighbors (Standard with K-optimization)
# 5. K-Nearest Neighbors (Iterative Imputation)
# 6. Random Forest (Iterative Imputation)
# 7. Gradient Boosting (Iterative Imputation)
# 8. Bayesian Ridge (Iterative Imputation)
# 9. Phylogenetic Tree-based imputation
#
# The iterative imputation methods use n_nearest_features of the most similar SNP sites to inform the imputation.
# Leaving n_nearest_features as None will use all sites, but it consumes a lot of memory and is not computationally tractable.
#
# For the phylogenetic tree-based method, you must supply an input tree, and it uses close-by samples to inform the imputation.
#
# To run the imputation, you just call GenotypeData.impute_missing() and supply the necessary arguments.
#
# You can supply a list of methods if you want to try multiple.
# Settings for each method can be supplied as a dictionary.
#
# impute_missing() is commented out here because I already ran the imputation.
#
# %% codecell
# Define imputation settings
#imputation_settings = {"br_n_iter": 1000, "n_nearest_features": 100}

# Impute missing data
#data.impute_missing(impute_methods="gb", impute_settings=imputation_settings)
# %% markdown
# Imputation can take a while, so if you've already run the imputation you can just re-load the imputed file.
# %% codecell
data.read_imputed("example_data/imputed/gb_iterative_imputed_012.csv", impute_methods="gb")
# %% markdown
# ## DimReduction Object
#
# Now you can define a DimReduction object to use with most of the downstream functions.
#
# You need to supply the 012-encoded genotypes, a list of population IDs, an output file prefix, and if you want replicates the reps argument.
#
# Fortunately, many attributes can be accessed from the GenotypeData object, including the imputed data as a pandas dataframe,
# the populations IDs, the sample IDs, and more.
#
# Additionally, you can define a custom color palette to use among many of the plots. Otherwise, it uses a default matplotlib color palette (default is "Set1"). The colors argument should just be a dictionary with unique population IDs as the keys and hex-code colors as the corresponding values.
#
# %% codecell
colors = {
    "GU": "#FF00FF",
    "EA": "#FF8C00",
    "TT": "#228B22",
    "TC": "#6495ED",
    "DS": "#00FFFF",
    "ON": "#800080",
    "CH": "#696969",
    "FL": "#FFFF00",
    "MX": "#FF0000"
}

dr = DimReduction(data.imputed_gb_df, data.populations, "test", reps=2, colors=colors)
# %% markdown
#
# ## Dimensionality Reduction
#
# There are several available methods for dimensionality reduction, including:
#
# 1. Principal Component Analysis (PCA)
# 2. Multidimensional Scaling (MDS)
# 3. t-Distributed Stochastic Neighbor Embedding (t-SNE)
# 4. Random Forest Embedding
#
# The inputs to each of these can also be first converted to a high-dimensionality random forest embedding.
# This essentially one-hot encodes the high-dimensionality data and in my experience improves the other embeddings.
#
# ### PCA
#
# So first, we're going to run a PCA. There is a plot_cumvar argument that allow you to inspect the cumulative explained variance for an initial PCA, and there are two methods available for choosing the optimal number of principal components to retain.
#
# The number of PCs to retain can be chosen by:
#
# 1. elbow method - Choose the inflection point where cumulative variance does not improve much with the addition of more PCs.
# 2. percentage of PCs
# 3. Manually
#
# You can specify whether to show the cumulative variance plot. Either way it gets saved to disk.
#
# There are
#
# Here we use the elbow method to get the inflection point.
#
# %% codecell
pca = runPCA(dimreduction=dr, plot_cumvar=True, show_cumvar=True, keep_pcs=data.indcount-1, elbow=True)
# %% codecell
